{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portuguese lemmatizer with spaCy\n",
    "\n",
    "This Jupyter notebook takes a text file, or folder of text files in Portuguese, and creates a set of lemmatized derivative files (where all words are in their dictionary form, and not inflected). These lemmatized files can then be used for searching, or other computational text analysis methods.\n",
    "\n",
    "For this notebook, we'll use the natural language processing (NLP) library spaCy, which can support multiple different languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First-time setup: spaCy library\n",
    "The code cells below below install the *spacy* package which can do the actual lemmatizing. You only need to run it the first time you use this notebook in a particular environment (laptop, virtual machine, etc.) You can skip it the next time you use the notebook, but nothing bad will happen if you re-run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\francisco\\anaconda3\\lib\\site-packages (3.2.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from spacy) (58.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from spacy) (21.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from spacy) (4.62.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from spacy) (2.26.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from spacy) (8.0.13)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from spacy) (0.7.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from spacy) (1.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from spacy) (1.20.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from spacy) (2.4.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.10.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "#Imports the module you need to download and install the spaCy modules\n",
    "import sys\n",
    "#Installs spaCy\n",
    "!{sys.executable} -m pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First-time setup: download language data\n",
    "The cell below downloads the Portuguese data for the spaCy NLP library. \n",
    "\n",
    "You can also check out [other language data available for spaCy](https://spacy.io/models), but you'll need to make a few other modifications to the code later on in order to use it. If you're new to Python, you might want to try one of the [other lemmatizer notebooks](https://github.com/quinnanya/dlcl204/tree/master/notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pt-core-news-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.2.0/pt_core_news_sm-3.2.0-py3-none-any.whl (22.2 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from pt-core-news-sm==3.2.0) (3.2.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (0.7.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (2.26.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (58.0.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-01 16:21:51.294236: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-02-01 16:21:51.294292: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (1.20.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (21.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (4.62.3)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\francisco\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (1.1.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('pt_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "!{sys.executable} -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing modules\n",
    "The next code cells imports the modules you need to run this notebook. Run them every time you open this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os is used to navigate your folder directories (e.g. change folders to where you files are stored)\n",
    "import os\n",
    "\n",
    "#imports spaCy\n",
    "import spacy\n",
    "\n",
    "#imports the Portuguese model\n",
    "import pt_core_news_sm\n",
    "\n",
    "#sets up a function so you can run the Portuguese model on texts\n",
    "ptnlp = pt_core_news_sm.load(disable = [\"ner\", \"tagger\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptnlp.max_length = 2557645 # or even higher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell below imports modules that are useful for cleaning texts, particularly texts converted from ebooks that include typographically fancy single and double quote characters that can break the lemmatization. It also sets up a function, `clean_string`, that we'll use on the text before running the lemmatizer.\n",
    "\n",
    "If your text doesn't have any of these problematic characters, nothing will happen (and that's okay)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports a module for converting Unicode characters to ASCII (English alphabet & punctuation)\n",
    "import unidecode\n",
    "#Imports a module with data about all the Unicode characters\n",
    "import unicodedata\n",
    "#Imports a module that does regular expressions, a kind of fancy find-and-replace syntax\n",
    "import re\n",
    "\n",
    "#Defines a character filter function\n",
    "def char_filter(string):\n",
    "    #Defines the set of Latin characters\n",
    "    latin = re.compile('[a-zA-Z]+')\n",
    "    #For each character in the text...\n",
    "    for char in unicodedata.normalize('NFC', string):\n",
    "        #Convert it to its ASCII equivalent\n",
    "        decoded = unidecode.unidecode(char)\n",
    "        #If the ASCII equivalent is a letter\n",
    "        if latin.match(decoded):\n",
    "            #Print the original character (so you don't lose accented letters)\n",
    "            yield char\n",
    "        #Otherwise...\n",
    "        else:\n",
    "            #Print the ASCII equivalent (e.g. standard quote character)\n",
    "            yield decoded\n",
    "\n",
    "#Defines a function for cleaning a text\n",
    "def clean_string(string):\n",
    "    #Runs the character filter function and reassmbles the text\n",
    "    return \"\".join(char_filter(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing a single file\n",
    "Put the full path to your text file in the cell below, using the correct syntax for your operating system. \n",
    "\n",
    "For instance, the default path a text file in the Documents directory is (substituting your user name on the computer for YOUR-USER-NAME):\n",
    "\n",
    "* On Mac: '/Users/YOUR-USER-NAME/Documents/YOUR-TEXT-FILE.txt'\n",
    "* On Windows: 'C:\\\\\\Users\\\\\\YOUR-USER-NAME\\\\\\Documents\\\\\\YOUR-TEXT-FILE.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Put the full path to your file between the single quotes here\n",
    "# filepath = 'C:\\Users\\Francisco\\Desktop\\Python\\FinalPaper\\Analysis\\MG.txt.txt'\n",
    "\n",
    "# #The outname is the name of the lemmatized file that this notebook creates\n",
    "# #If you want it to be named something other than the original file name + -lemmatized\n",
    "# #you can change that here\n",
    "# outname = filepath.replace('.txt', '-lemmatized.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Opens the file you specified\n",
    "# with open(filepath, 'r', encoding='utf8') as f:\n",
    "#     #Creates an empty text file with -lemmatized.txt appended to the name\n",
    "#     with open(outname, 'w', encoding='utf8') as out:\n",
    "#         #Reads the text of the file you specified\n",
    "#         text = f.read()\n",
    "#         #Removes any problematic punctuation\n",
    "#         cleantext = clean_string(text)\n",
    "#         #Does Portuguese NLP on the cleaned text\n",
    "#         doc = ptnlp(cleantext)\n",
    "#         #For each word in the text...\n",
    "#         for token in doc:\n",
    "#             #Write the lemma to the new text file with the lemmatized text\n",
    "#             out.write(token.lemma_)\n",
    "#             #Write a space after each word\n",
    "#             out.write(' ')\n",
    "#             #Print the lemmas to the screen below, with a space between them\n",
    "#             print(token.lemma_, end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing a folder of text files\n",
    "Put the full path to your folder of text files in the cell below, using the correct syntax for your operating system. \n",
    "\n",
    "For instance, the default path to a folder called \"portuguese\" in the Documents directory is (substituting your user name on the computer for YOUR-USER-NAME):\n",
    "\n",
    "* On Mac: '/Users/YOUR-USER-NAME/Documents/portuguese'\n",
    "* On Windows: 'C:\\\\\\Users\\\\\\YOUR-USER-NAME\\\\\\Documents\\\\\\portuguese'\n",
    "\n",
    "With a whole folder of texts, printing the full text to the screen can make your Jupyter notebook file get very big, so it's been \"turned off\" here. If you want to see the script at work, you can remove the `#` character before the `#print(token.lemma_, end=' ')` at the end of the second cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put the full path to your folder between single quotes here\n",
    "textfolder = r'C:\\Users\\Francisco\\Desktop\\Python\\FinalPaper\\Analysis'\n",
    "#Changes the working directory to the folder you specified\n",
    "os.chdir(textfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptnlp.Defaults.stop_words |= {\"Rede Estadual\", \"estadual de educação\", \"Estadual de Educação\", \"ESTADUAL DE EDUCAÇÃO\", \"unidade curricular\", \"ensino médio\", \"rede estadual\", \"governo\", \"secretaria\", \"secretaria educação\", \"base nacional\", \"comum curricular\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For every file in the folder you specified...\n",
    "for filename in os.listdir(textfolder):\n",
    "    #If it's a text file, but not one of the text files with just lemmas\n",
    "    if filename.endswith('.txt') and not filename.endswith('-lemmatized.txt'):\n",
    "        #The outname is the name of the lemmatized file that this notebook creates\n",
    "        #If you want it to be named something other than the original file name + -lemmatized\n",
    "        #you can change that here\n",
    "        outname = filename.replace('.txt', '-lemmatized.txt')\n",
    "        #Opens the file you specified\n",
    "        with open(filename, 'r', encoding='utf8') as f:\n",
    "            #Creates an empty text file with -lemmatized.txt appended to the name\n",
    "            with open(outname, 'w', encoding='utf8') as out:\n",
    "                #Reads the text of the file you specified\n",
    "                text = f.read()\n",
    "                #Removes any problematic punctuation\n",
    "                cleantext = clean_string(text)\n",
    "                #Does Portuguese NLP on the cleaned text\n",
    "                doc = ptnlp(cleantext)\n",
    "                #For each word in the text...\n",
    "                for token in doc:\n",
    "                    if token.is_stop == False and token.is_punct == False and token.is_alpha == True:\n",
    "                    #Write the lemma to the new text file with the lemmatized text\n",
    "                        out.write(token.lemma_)\n",
    "                    #Write a space after each word\n",
    "                        out.write(' ')\n",
    "                    #Print the lemmas to the screen below, with a space between them\n",
    "                    #print(token.lemma_, end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About\n",
    "\n",
    "This Jupyter notebook was originally developed by Quinn Dombrowski for use in [DLCL 204: Digital Humanities Across Borders](https://github.com/quinnanya/dlcl204) at Stanford University, fall 2020. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
